{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd60c4c-fefa-4839-8fec-7c434b43d525",
   "metadata": {},
   "source": [
    "# Model Training Data Preparation \n",
    "\n",
    "In this notebook, we perform the following tasks:\n",
    "\n",
    "    1. Down features data files in data14group1-ml S3 bucket(from ETL job).\n",
    "    2. Create a Spark dataframe and audit the data set\n",
    "    3. Split (randomly) the data set into (train:80%, validation:20%)\n",
    "    4. Delete the hidden files (sagemaker estimator does not like these files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96716172-e110-4e14-abd9-2ec9fcd99a38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6143bc55-86ce-4d92-ad76-f40ace104c59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store the current SageMaker session\n",
    "session = sagemaker.Session()\n",
    "# Define the global bucket name\n",
    "bucket = \"data14group1-ml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa68f7-72de-4421-b57d-614dcb2fb73c",
   "metadata": {},
   "source": [
    "### Download trainval data parquet files from S3 bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c188eab-97d1-4c2c-94cb-0143f98e16c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data/trainval/part-00000-45907027-bb5a-46b1-bc94-25f1cd988849-c000.snappy.parquet to data/part-00000-45907027-bb5a-46b1-bc94-25f1cd988849-c000.snappy.parquet\n",
      "Downloaded data/trainval/part-00001-45907027-bb5a-46b1-bc94-25f1cd988849-c000.snappy.parquet to data/part-00001-45907027-bb5a-46b1-bc94-25f1cd988849-c000.snappy.parquet\n",
      "Downloaded data/trainval/part-00002-45907027-bb5a-46b1-bc94-25f1cd988849-c000.snappy.parquet to data/part-00002-45907027-bb5a-46b1-bc94-25f1cd988849-c000.snappy.parquet\n",
      "Downloaded data/trainval/part-00003-45907027-bb5a-46b1-bc94-25f1cd988849-c000.snappy.parquet to data/part-00003-45907027-bb5a-46b1-bc94-25f1cd988849-c000.snappy.parquet\n",
      "\n",
      "data download complete\n"
     ]
    }
   ],
   "source": [
    "# Create a Boto3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Define S3 bucket and folder path\n",
    "bucket_name = \"data14group1-ml\"\n",
    "s3_folder_path = 'data/trainval/'\n",
    "\n",
    "# Define the local directory where files will be downloaded\n",
    "local_dir = \"data/\"\n",
    "\n",
    "# Ensure the local directory exists\n",
    "if not os.path.exists(local_dir):\n",
    "    os.makedirs(local_dir)\n",
    "\n",
    "# List all objects in the S3 folder\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=s3_folder_path)\n",
    "\n",
    "# Iterate through the objects and download each one\n",
    "for obj in response.get('Contents', []):\n",
    "    # Get the file path\n",
    "    s3_file_path = obj['Key']\n",
    "    file_name = os.path.basename(s3_file_path)\n",
    "\n",
    "    # Define the local file path\n",
    "    local_file_path = os.path.join(local_dir, file_name)\n",
    "\n",
    "    # Download the file from S3\n",
    "    s3_client.download_file(bucket_name, s3_file_path, local_file_path)\n",
    "    print(f'Downloaded {s3_file_path} to {local_file_path}')\n",
    "\n",
    "print(\"\\ndata download complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380d9f9-f150-4e45-9b67-cbb65befea91",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Split data into train, valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "879fdbf8-3883-4f3c-b5ba-4585b4a225af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loading complete\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ReadParquetFromData\").getOrCreate()\n",
    "\n",
    "# Define file path\n",
    "file_path = \"data/\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "data = spark.read.parquet(file_path)\n",
    "\n",
    "print(\"data loading complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8883888f-8f71-41ed-8463-3233d47e9f15",
   "metadata": {},
   "source": [
    "### Check data integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99007a1d-e82c-441f-9981-eca0c4d601fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data number of rows: 8474661\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, col\n",
    "\n",
    "# Check data count (8474661)\n",
    "print(f\"data number of rows: {data.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2ae7fb-2be5-4689-91e2-7d36e89f7c5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 115:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaN values detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the columns that contain NaN value\n",
    "nan_count = 0\n",
    "for c in data.columns:\n",
    "    count = 0\n",
    "    count = data.filter(isnan(col(c))).count()\n",
    "    if count > 0:\n",
    "        nan_count += 1\n",
    "        print(f\"There are {count} records has NaN value for column: {c}\")\n",
    "if nan_count == 0:\n",
    "    print(\"No NaN values detected\")\n",
    "else:\n",
    "    print(\"Check records with NaN values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c2964-baf9-46d6-8ead-1861d2fda1d5",
   "metadata": {},
   "source": [
    "### Split data into train set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06c322d0-4329-4169-876f-0b62b8e76bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train and validation sets are randomly selected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6778638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 133:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1696023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split the data into training (80%) and validation (20%) sets\n",
    "seed = 42\n",
    "train, validation = data.randomSplit([0.8,0.2], seed=seed)\n",
    "print(\"train and validation sets are randomly selected\")\n",
    "\n",
    "print(train.count())\n",
    "print(validation.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75896524-2481-4f4d-a9d2-4c83585fdc6d",
   "metadata": {},
   "source": [
    "### Save train and validation sets to local folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60a3d249-fb29-4350-bb20-8cecb34649f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 136:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train file saving complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "file_path = \"train/\"\n",
    "train.write.mode(\"overwrite\").parquet(file_path)\n",
    "print(\"train file saving complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f6f663-7525-4dde-8df9-9ec90d9e6df9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 137:============================>                            (2 + 2) / 4]\r"
     ]
    }
   ],
   "source": [
    "file_path = \"validation/\"\n",
    "validation.write.mode(\"overwrite\").parquet(file_path)\n",
    "print(\"validation file saving complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42cf374-b336-41cc-8902-0c4b79a7ad56",
   "metadata": {},
   "source": [
    "### Save train and validation sets to local folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d026f8b2-3ef0-47e2-b6d4-8d615cd1b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"train/\"\n",
    "train.write.mode(\"overwrite\").parquet(file_path)\n",
    "print(\"train file saving complete\")\n",
    "\n",
    "file_path = \"validation/\"\n",
    "validation.write.mode(\"overwrite\").parquet(file_path)\n",
    "print(\"validation file saving complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea44f02-f208-4457-9d5d-32c26df3c88b",
   "metadata": {},
   "source": [
    "### Upload to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "544d2b51-a545-4bc7-8e72-ca03f3691097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload complete\n"
     ]
    }
   ],
   "source": [
    "prefix = \"data\"\n",
    "session.upload_data(\n",
    "    \"train/\", bucket=bucket, key_prefix=f\"{prefix}/train\"\n",
    ")\n",
    "\n",
    "session.upload_data(\n",
    "    \"validation/\", bucket=bucket, key_prefix=f\"{prefix}/validation\"\n",
    ")\n",
    "print(\"upload complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ac543-73a3-4ade-8ed3-babd59108cc5",
   "metadata": {},
   "source": [
    "### Make sure delete hidden files in the train and validation folder in S3 before training\n",
    "\n",
    "Model training won't work if we don't delete them!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6e2b8d3-63c4-45d3-a713-5ba7dc22a423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting data/train/._SUCCESS.crc\n",
      "Deleting data/train/.part-00000-1c50e827-e50f-4982-aa06-c4eae4d74e91-c000.snappy.parquet.crc\n",
      "Deleting data/train/.part-00001-1c50e827-e50f-4982-aa06-c4eae4d74e91-c000.snappy.parquet.crc\n",
      "Deleting data/train/.part-00002-1c50e827-e50f-4982-aa06-c4eae4d74e91-c000.snappy.parquet.crc\n",
      "Deleting data/train/.part-00003-1c50e827-e50f-4982-aa06-c4eae4d74e91-c000.snappy.parquet.crc\n",
      "Deleting data/validation/._SUCCESS.crc\n",
      "Deleting data/validation/.part-00000-c3087eba-79fd-4c30-b3fc-65f1a2efc3fd-c000.snappy.parquet.crc\n",
      "Deleting data/validation/.part-00001-c3087eba-79fd-4c30-b3fc-65f1a2efc3fd-c000.snappy.parquet.crc\n",
      "Deleting data/validation/.part-00002-c3087eba-79fd-4c30-b3fc-65f1a2efc3fd-c000.snappy.parquet.crc\n",
      "Deleting data/validation/.part-00003-c3087eba-79fd-4c30-b3fc-65f1a2efc3fd-c000.snappy.parquet.crc\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "# Specify your bucket name and prefix\n",
    "bucket = \"data14group1-ml\"\n",
    "prefix_train = \"data/train\"\n",
    "prefix_validation = \"data/validation\"\n",
    "\n",
    "def delete_crc(bucket_name, prefix):\n",
    "    # List and delete .crc files\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            if key.endswith('.crc'):\n",
    "                print(f'Deleting {key}')\n",
    "                s3_client.delete_object(Bucket=bucket_name, Key=key)\n",
    "\n",
    "delete_crc(bucket, prefix_train)\n",
    "delete_crc(bucket, prefix_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81fe4b2-53d4-4973-8ff4-1ec1f205dbc5",
   "metadata": {},
   "source": [
    "### Delete local data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59afdd67-58a8-474b-8e62-8fbb1c607d74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -r data train validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
